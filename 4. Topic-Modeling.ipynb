{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8103c45e",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "## Find various topics that are present in your corpus. Each document in the corpus will be made up of at least one topic. \n",
    "## Latent Dirichlet Allocation(LDA)\n",
    "## need to proivde: document-term matrix and numeber of topics you would like the algorithm to pick up\n",
    "## Once the topic modelling technique is applied, your job as a human is to interpret the results and see if the mix of words in each topic make sense. If they don't make sense, you can try changing up the number of topics, the term in the document-term matrix, model parameters, or even try a different model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223449bc",
   "metadata": {},
   "source": [
    "# Topic Modeling - Attempt #1(All Text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda5117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read in our document-term matrix\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "data = pd.read_pickle('dtm_stop.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0ab116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules for LDA with gensim\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363494af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of the required inputs is a term-document matrix\n",
    "\n",
    "tdm = data.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f34225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256a2faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim also requires dictionary of the all terms and their representive location in the term-document matrix\n",
    "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817530d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the corpus(term-document matrix) and id2word (dictionary of location: term),\n",
    "# We need to specify two other parameters as well - the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeff4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA for num_topics = 3\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes = 10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdbc35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA for num_topics = 4\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3a3d0e",
   "metadata": {},
   "source": [
    "# Topic Modeling - Attempt #2(Nouns Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42780cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a funtion to pull out nouns from a string of text\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos:pos[:2]=='NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)]\n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e75c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the cleaned data, before the CountVectorizer step\n",
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8bf583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the nouns function to the transcript to filter only on nouns\n",
    "data_nouns = pd.DataFrame(data_clean.transcript.apply(nouns))\n",
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbf8565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Re-add the additional stop words since we are recreating the document-term matrix \n",
    "add_stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people',\n",
    "                  'youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words=stop_words)\n",
    "data_cvn = cvn.fit_transform(data_nouns.transcript)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns = cvn.get_feature_names())\n",
    "data_dtmn.index = data.nouns.index\n",
    "data_dtmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde96d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400fab08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with 2 topics\n",
    "ldan = models.LdaModel(corpus=corposn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5698c8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try topics = 3\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=3, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1329ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try 4 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a48565",
   "metadata": {},
   "source": [
    "# Topic Modeling - Attempt #3(Nouns and Adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e3c3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos:pos[:2] == 'NN' or pos[:2]== 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)]\n",
    "    return ' '.join(noun_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843595f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns_adj = pd.DataFrame(data_clean.transcript.apply(nouns_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31efcd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new document-term matrix using only nouns and adjective, also remove common words with max_df\n",
    "cvna = CountVectorizer(stop_words = stop_words, max_df = .8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.transcript)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "data_damna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee779b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2word = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6234c3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus = corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c05b2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try 3 topics\n",
    "ldana = models.LdaModel(corpus = corpusna, num_topics =3, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d7a923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try 4 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics = 3, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bed0e4",
   "metadata": {},
   "source": [
    "# Identify Topics in Each DocumentÂ¶\n",
    "\n",
    "## Out of the 9 topic models we looked at, the nouns and adjectives, 4 topic one made the most sense. So let's pull that down here and run it through some more iterations to get more fine-tuned topics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359f33bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our final LDA model(for now)\n",
    "ldana = models.LdaModel(corpus=corpus, num_topics=4, id2word=idwordna, passes=80)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebec505",
   "metadata": {},
   "source": [
    "[(0,\n",
    "  '0.009*\"joke\" + 0.005*\"mom\" + 0.005*\"parents\" + 0.004*\"hasan\" + 0.004*\"jokes\" + 0.004*\"anthony\" + 0.003*\"nuts\" + 0.003*\"dead\" + 0.003*\"tit\" + 0.003*\"twitter\"'),\n",
    " (1,\n",
    "  '0.005*\"mom\" + 0.005*\"jenny\" + 0.005*\"clinton\" + 0.004*\"friend\" + 0.004*\"parents\" + 0.003*\"husband\" + 0.003*\"cow\" + 0.003*\"ok\" + 0.003*\"wife\" + 0.003*\"john\"'),\n",
    " (2,\n",
    "  '0.005*\"bo\" + 0.005*\"gun\" + 0.005*\"guns\" + 0.005*\"repeat\" + 0.004*\"um\" + 0.004*\"ass\" + 0.004*\"eye\" + 0.004*\"contact\" + 0.003*\"son\" + 0.003*\"class\"'),\n",
    " (3,\n",
    "  '0.006*\"ahah\" + 0.004*\"nigga\" + 0.004*\"gay\" + 0.003*\"dick\" + 0.003*\"door\" + 0.003*\"young\" + 0.003*\"motherfucker\" + 0.003*\"stupid\" + 0.003*\"bitch\" + 0.003*\"mad\"')]\n",
    "These four topics look pretty decent. Let's settle on these for now.\n",
    "\n",
    "Topic 0: mom, parents\n",
    "Topic 1: husband, wife\n",
    "Topic 2: guns\n",
    "Topic 3: profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880593d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at which topics each transcript contains\n",
    "corpus_transformed = ldana[corpusna]\n",
    "list(zip([a for [(a,b)] in corpus_transformed], data_dtmna.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c33c4f",
   "metadata": {},
   "source": [
    "[(1, 'ali'),\n",
    " (0, 'anthony'),\n",
    " (2, 'bill'),\n",
    " (2, 'bo'),\n",
    " (3, 'dave'),\n",
    " (0, 'hasan'),\n",
    " (2, 'jim'),\n",
    " (3, 'joe'),\n",
    " (1, 'john'),\n",
    " (0, 'louis'),\n",
    " (1, 'mike'),\n",
    " (0, 'ricky')]\n",
    "For a first pass of LDA, these kind of make sense to me, so we'll call it a day for now.\n",
    "\n",
    "Topic 0: mom, parents [Anthony, Hasan, Louis, Ricky]\n",
    "Topic 1: husband, wife [Ali, John, Mike]\n",
    "Topic 2: guns [Bill, Bo, Jim]\n",
    "Topic 3: profanity [Dave, Joe]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
