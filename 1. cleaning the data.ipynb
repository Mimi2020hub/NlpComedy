{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bda1d7bf",
   "metadata": {},
   "source": [
    "# Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516a52b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web scraping, pickle imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "\n",
    "# Scrape transcript data from scrapesfromtheloft.com\n",
    "def url_to_transcript(url):\n",
    "    '''Returns transcript data specifically from scrapesfromtheloft.com'''\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    text = [p.text for p in soup.find(calss_=\"post-content\").find_all('p')]\n",
    "    print(url)\n",
    "    return text\n",
    "\n",
    "# URLs of transcripts in scope\n",
    "urls = ['http://scrapsfromtheloft.com/2017/05/06/louis-ck-oh-my-god-full-transcript/',\n",
    "        'http://scrapsfromtheloft.com/2017/04/11/dave-chappelle-age-spin-2017-full-transcript/',\n",
    "        'http://scrapsfromtheloft.com/2018/03/15/ricky-gervais-humanity-transcript/',\n",
    "        'http://scrapsfromtheloft.com/2017/08/07/bo-burnham-2013-full-transcript/',\n",
    "        'http://scrapsfromtheloft.com/2017/05/24/bill-burr-im-sorry-feel-way-2014-full-transcript/',\n",
    "        'http://scrapsfromtheloft.com/2017/04/21/jim-jefferies-bare-2014-full-transcript/',\n",
    "        'http://scrapsfromtheloft.com/2017/08/02/john-mulaney-comeback-kid-2015-full-transcript/',\n",
    "        'http://scrapsfromtheloft.com/2017/10/21/hasan-minhaj-homecoming-king-2017-full-transcript/',\n",
    "        'http://scrapsfromtheloft.com/2017/09/19/ali-wong-baby-cobra-2016-full-transcript/',\n",
    "        'http://scrapsfromtheloft.com/2017/08/03/anthony-jeselnik-thoughts-prayers-2015-full-transcript/',\n",
    "        'http://scrapsfromtheloft.com/2018/03/03/mike-birbiglia-my-girlfriends-boyfriend-2013-full-transcript/',\n",
    "        'http://scrapsfromtheloft.com/2017/08/19/joe-rogan-triggered-2016-full-transcript/']\n",
    "\n",
    "# Comedian names\n",
    "comedians = ['louis', 'dave', 'ricky', 'bo', 'bill', 'jim', 'john', 'hasan', 'ali', 'anthony', 'mike', 'joe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8bcf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually request transcript \n",
    "transcripts = [url_to_transcript(u) for u in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6da9ebd",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-b1fb300b5736>, line 62)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-b1fb300b5736>\"\u001b[0;36m, line \u001b[0;32m62\u001b[0m\n\u001b[0;31m    with open(\"transcripts/\" + c + )\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Pickle files for later use\n",
    "# # Make a new directory to hold the text files\n",
    "# !mkdir transcripts\n",
    "\n",
    "for i, c in enumerate(comedians):\n",
    "    with open(\"transcripts/\" + c + '.txt', \"wb\") as file:\n",
    "        pickle.dump(transcripts[i], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14fa5b54",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-ce8fdd2c3181>, line 39)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-ce8fdd2c3181>\"\u001b[0;36m, line \u001b[0;32m39\u001b[0m\n\u001b[0;31m    with open(\"transcripts/\"  + c + \"\")\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Load pickled files\n",
    "data = {}\n",
    "for i, c in enumerate(comedians):\n",
    "    with open(\"transcripts/\" + c + \".txt\", \"rb\") as file:\n",
    "        data[c] = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfda6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check to make sure data has been loaded properly\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e40e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More checks\n",
    "data['louis'][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f137322c",
   "metadata": {},
   "source": [
    "# Cleanig the data\n",
    "\n",
    "## Minimum viable product approach - start simple and iterate\n",
    "\n",
    "## Common data cleaning steps on all text\n",
    "### Make text all lower case\n",
    "### Remove punctuation\n",
    "### Remove numerical values\n",
    "### Remove common non-sensical text(/n)\n",
    "## Tokenize text\n",
    "### Remove stop words\n",
    "\n",
    "## More data cleaning steps after tokenization:\n",
    "### Stemming / lemmatization\n",
    "### Parts of speech tagging\n",
    "### Create bi-grams or tri-grams\n",
    "### Deal with typos\n",
    "### And more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff47705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at our data again\n",
    "next(iter(data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a01613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that our dictionary is currently in key: comedian, value: list of text format\n",
    "next(iter(data.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64865c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to change this to key: comedian, value: string format\n",
    "def combine_text(list_of_text):\n",
    "    '''Takes a list of text and combines them into one large chunk of text.'''\n",
    "    combined_text = ' '.join(list_of_text)\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e89e30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine it!\n",
    "data_combined = {key:[combine_text(value)] for (key, value) in data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a947c0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can either keep it in dictionary format or put it into a pandas dataframe\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', 150)\n",
    "\n",
    "data_df =pd.DataFrame.from_dict(data_combined).transpose()\n",
    "data_df.columns = ['transcript']\n",
    "data_df = data_df.sort_index()\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e059bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the transcript for Ali Wong\n",
    "data_df.transcript.loc['ali']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7201ab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a first round of text cleaning techniques\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]','', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "round1 = lambda x: clean_text_round1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45b3aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the updated text\n",
    "data_clean = pd.DataFrame(data_df.tanscript.apply(round1))\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39756625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a second round of cleaning \n",
    "def clean_text_round2(text):\n",
    "    '''Get rid of some additional punctuation and non-sensical text that was missed the first around.'''\n",
    "    text = re.sub('[''\"\"...]', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    return text\n",
    "round2 = lambda x:clean_text_round2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc1d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the updated text\n",
    "data_clean = pd.DataFrame(data_clean.transcript.apply(round2))\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea67d92",
   "metadata": {},
   "source": [
    "# Orgnizing the data\n",
    "## Corpus: a collection of text\n",
    "## Document-Term Matrix - word counts in matrix format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f900fd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at our dataframe\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f80df2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add the comedians' full names as well\n",
    "full_names = ['Ali Wong', 'Anthony Jeselnik', 'Bill Burr', 'Bo Burnham', 'Dave Chappelle', 'Hasan Minhaj',\n",
    "              'Jim Jefferies', 'Joe Rogan', 'John Mulaney', 'Louis C.K.', 'Mike Birbiglia', 'Ricky Gervais']\n",
    "data_df['full_name'] = full_names\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4677ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pickle it for later use\n",
    "data_df.to_pickle(\"corpus.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81edc58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to create a document-term matrix using CountVectorizer, and exclude common English stop words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words = 'english')\n",
    "data_cv = cv.fit_transform(data_clean.transcript)\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = data_clean.index\n",
    "data_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9224e0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pickle it for later use\n",
    "data_dtm.to_pickle(\"dtm.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ede165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also pickle the cleaned data (before we put it in document-term matrix format) and the CountVectorizer object\n",
    "data_clean.to_pickle('data_clean.pkl')\n",
    "pickle.dump(cv, open(\"cv.pkl\"), \"wb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
